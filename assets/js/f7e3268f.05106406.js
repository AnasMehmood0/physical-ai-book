"use strict";(globalThis.webpackChunkweb=globalThis.webpackChunkweb||[]).push([[5344],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}},8621:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla/whisper-voice","title":"01. Whisper Voice","description":"Large Language Models for Voice Control","source":"@site/docs/vla/whisper-voice.md","sourceDirName":"vla","slug":"/vla/whisper-voice","permalink":"/physical-ai-book/docs/vla/whisper-voice","draft":false,"unlisted":false,"editUrl":"https://github.com/AnasMehmood0/physical-ai-book/tree/main/web/docs/vla/whisper-voice.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"01. Whisper Voice"},"sidebar":"tutorialSidebar","previous":{"title":"02. Reinforcement Learning","permalink":"/physical-ai-book/docs/isaac/reinforcement-learning"},"next":{"title":"02. Capstone Project","permalink":"/physical-ai-book/docs/vla/capstone-project"}}');var t=i(4848),s=i(8453);const a={sidebar_position:1,sidebar_label:"01. Whisper Voice"},r="01. Whisper Voice",l={},c=[{value:"Large Language Models for Voice Control",id:"large-language-models-for-voice-control",level:2},{value:"Overview of Voice-Language-Action (VLA) Models",id:"overview-of-voice-language-action-vla-models",level:2},{value:"Components",id:"components",level:3},{value:"Whisper: Robust Speech Recognition",id:"whisper-robust-speech-recognition",level:2},{value:"What is Whisper?",id:"what-is-whisper",level:3},{value:"Installation and Usage",id:"installation-and-usage",level:3},{value:"LLM Integration for Command Understanding",id:"llm-integration-for-command-understanding",level:2},{value:"Using Gemini API",id:"using-gemini-api",level:3},{value:"Action Grounding",id:"action-grounding",level:3},{value:"Complete Voice Control System",id:"complete-voice-control-system",level:2},{value:"Handling Ambiguity and Safety",id:"handling-ambiguity-and-safety",level:2},{value:"Clarification Dialogs",id:"clarification-dialogs",level:3},{value:"Multimodal Understanding",id:"multimodal-understanding",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"01-whisper-voice",children:"01. Whisper Voice"})}),"\n",(0,t.jsx)(n.h2,{id:"large-language-models-for-voice-control",children:"Large Language Models for Voice Control"}),"\n",(0,t.jsx)(n.p,{children:"This chapter explores the integration of Large Language Models (LLMs) with voice control systems, focusing on technologies like OpenAI's Whisper for robust speech-to-text transcription and subsequent processing by advanced LLMs. It covers the principles of voice command recognition, natural language understanding (NLU), and the generation of appropriate robotic responses or actions."}),"\n",(0,t.jsx)(n.h2,{id:"overview-of-voice-language-action-vla-models",children:"Overview of Voice-Language-Action (VLA) Models"}),"\n",(0,t.jsx)(n.p,{children:"VLA models combine speech recognition, language understanding, and action grounding to enable natural voice control of robots."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Voice Input \u2192 Speech-to-Text (Whisper) \u2192 Language Understanding (LLM) \u2192 Action Grounding \u2192 Robot Execution\n"})}),"\n",(0,t.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"1. Speech Recognition (Whisper)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Converts audio to text"}),"\n",(0,t.jsx)(n.li,{children:"Robust to accents, noise, multilingual"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"2. Language Understanding (LLM)"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Interprets user intent"}),"\n",(0,t.jsx)(n.li,{children:"Handles ambiguity and context"}),"\n",(0,t.jsx)(n.li,{children:"Generates actionable commands"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"3. Action Grounding"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Maps language to robot primitives"}),"\n",(0,t.jsx)(n.li,{children:"Validates feasibility"}),"\n",(0,t.jsx)(n.li,{children:"Generates motion plans"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"4. Execution"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Sends commands to robot controllers"}),"\n",(0,t.jsx)(n.li,{children:"Monitors execution"}),"\n",(0,t.jsx)(n.li,{children:"Provides feedback to user"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"whisper-robust-speech-recognition",children:"Whisper: Robust Speech Recognition"}),"\n",(0,t.jsx)(n.h3,{id:"what-is-whisper",children:"What is Whisper?"}),"\n",(0,t.jsx)(n.p,{children:"Whisper is OpenAI's open-source speech recognition model trained on 680,000 hours of multilingual data."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Models"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"tiny"}),": 39M params, fastest"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"base"}),": 74M params"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"small"}),": 244M params"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"medium"}),": 769M params"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"large"}),": 1550M params, most accurate"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"installation-and-usage",children:"Installation and Usage"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Basic Usage"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load model\nmodel = whisper.load_model("base")\n\n# Transcribe audio\nresult = model.transcribe("audio.mp3")\nprint(result["text"])\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Real-time Transcription"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport pyaudio\nimport wave\nimport tempfile\nimport os\n\nclass RealtimeTranscriber:\n    def __init__(self, model_size="base"):\n        self.model = whisper.load_model(model_size)\n        self.audio = pyaudio.PyAudio()\n\n        self.chunk = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000\n        self.record_seconds = 3\n\n    def record_audio(self):\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        print("Listening...")\n        frames = []\n\n        for _ in range(0, int(self.rate / self.chunk * self.record_seconds)):\n            data = stream.read(self.chunk)\n            frames.append(data)\n\n        stream.stop_stream()\n        stream.close()\n\n        return b\'\'.join(frames)\n\n    def transcribe(self, audio_data):\n        # Save to temporary file\n        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:\n            wf = wave.open(f.name, \'wb\')\n            wf.setnchannels(self.channels)\n            wf.setsampwidth(self.audio.get_sample_size(self.format))\n            wf.setframerate(self.rate)\n            wf.writeframes(audio_data)\n            wf.close()\n\n            # Transcribe\n            result = self.model.transcribe(f.name)\n            os.unlink(f.name)\n\n            return result["text"]\n\n    def listen_and_transcribe(self):\n        audio_data = self.record_audio()\n        text = self.transcribe(audio_data)\n        print(f"Transcribed: {text}")\n        return text\n\n# Usage\ntranscriber = RealtimeTranscriber(model_size="base")\n\nwhile True:\n    command = transcriber.listen_and_transcribe()\n    if "stop listening" in command.lower():\n        break\n'})}),"\n",(0,t.jsx)(n.h2,{id:"llm-integration-for-command-understanding",children:"LLM Integration for Command Understanding"}),"\n",(0,t.jsx)(n.h3,{id:"using-gemini-api",children:"Using Gemini API"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import google.generativeai as genai\nimport os\n\ngenai.configure(api_key=os.getenv("GEMINI_API_KEY"))\n\nclass VoiceCommandProcessor:\n    def __init__(self):\n        self.model = genai.GenerativeModel(\'gemini-2.0-flash-exp\')\n        self.conversation_history = []\n\n    def process_command(self, voice_input):\n        # Add context about robot capabilities\n        system_prompt = """You are a robot control assistant. The robot can:\n        - Navigate to locations: "go to [location]"\n        - Pick up objects: "pick up [object]"\n        - Place objects: "place [object] on [location]"\n        - Move joints: "move [joint] to [angle] degrees"\n        - Execute sequences: "first [action], then [action]"\n\n        Convert natural language commands into structured JSON actions.\n        """\n\n        user_message = f"{system_prompt}\\n\\nUser command: {voice_input}\\n\\nGenerate JSON action:"\n\n        response = self.model.generate_content(user_message)\n\n        return response.text\n\n# Example usage\nprocessor = VoiceCommandProcessor()\n\nvoice_input = "Please pick up the red cup and put it on the table"\naction = processor.process_command(voice_input)\nprint(action)\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{\n  "actions": [\n    {\n      "type": "navigate",\n      "target": "red cup",\n      "method": "visual_search"\n    },\n    {\n      "type": "grasp",\n      "object": "red cup",\n      "approach": "top_down"\n    },\n    {\n      "type": "navigate",\n      "target": "table"\n    },\n    {\n      "type": "place",\n      "object": "red cup",\n      "location": "table"\n    }\n  ]\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"action-grounding",children:"Action Grounding"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import json\nimport numpy as np\n\nclass ActionGrounder:\n    def __init__(self, robot_interface):\n        self.robot = robot_interface\n        self.object_positions = {}  # From perception system\n\n    def ground_action(self, action_json):\n        action = json.loads(action_json)\n\n        for step in action.get("actions", []):\n            action_type = step["type"]\n\n            if action_type == "grasp":\n                self.execute_grasp(step)\n            elif action_type == "place":\n                self.execute_place(step)\n            elif action_type == "navigate":\n                self.execute_navigate(step)\n            elif action_type == "move_joint":\n                self.execute_move_joint(step)\n\n    def execute_grasp(self, action):\n        object_name = action["object"]\n\n        # Get object position from perception\n        if object_name not in self.object_positions:\n            print(f"Cannot find {object_name}")\n            return False\n\n        pos = self.object_positions[object_name]\n\n        # Move to pre-grasp position\n        pre_grasp = pos + np.array([0, 0, 0.1])\n        self.robot.move_to(pre_grasp)\n\n        # Approach object\n        self.robot.move_to(pos)\n\n        # Close gripper\n        self.robot.gripper.close()\n\n        # Lift\n        self.robot.move_to(pos + np.array([0, 0, 0.1]))\n\n        return True\n\n    def execute_place(self, action):\n        location = action["location"]\n        target_pos = self.object_positions.get(location, np.array([0, 0, 0]))\n\n        # Move to above placement location\n        above_target = target_pos + np.array([0, 0, 0.2])\n        self.robot.move_to(above_target)\n\n        # Lower to placement height\n        self.robot.move_to(target_pos + np.array([0, 0, 0.05]))\n\n        # Open gripper\n        self.robot.gripper.open()\n\n        # Retract\n        self.robot.move_to(above_target)\n\n        return True\n\n    def execute_navigate(self, action):\n        target = action["target"]\n\n        if target in self.object_positions:\n            self.robot.navigate_to(self.object_positions[target])\n        else:\n            print(f"Unknown location: {target}")\n\n    def execute_move_joint(self, action):\n        joint_name = action["joint"]\n        angle = action["angle"]\n\n        self.robot.move_joint(joint_name, np.deg2rad(angle))\n'})}),"\n",(0,t.jsx)(n.h2,{id:"complete-voice-control-system",children:"Complete Voice Control System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport whisper\nimport google.generativeai as genai\nimport json\n\nclass VoiceControlNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_control_node\')\n\n        # Publishers\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.status_pub = self.create_publisher(String, \'/robot/status\', 10)\n\n        # Initialize Whisper\n        self.whisper_model = whisper.load_model("base")\n\n        # Initialize Gemini\n        genai.configure(api_key=os.getenv("GEMINI_API_KEY"))\n        self.llm = genai.GenerativeModel(\'gemini-2.0-flash-exp\')\n\n        # Transcriber\n        self.transcriber = RealtimeTranscriber()\n\n        # Start voice control loop\n        self.timer = self.create_timer(0.1, self.voice_control_loop)\n\n        self.get_logger().info(\'Voice control node started\')\n\n    def voice_control_loop(self):\n        # Listen for command\n        command = self.transcriber.listen_and_transcribe()\n\n        if not command:\n            return\n\n        self.get_logger().info(f\'Heard: {command}\')\n\n        # Process with LLM\n        action = self.process_command(command)\n\n        # Execute action\n        self.execute_action(action)\n\n    def process_command(self, command):\n        prompt = f"""Convert this robot command to JSON:\n        "{command}"\n\n        Available actions:\n        - move_forward: {{"type": "move", "direction": "forward", "speed": 0.5}}\n        - move_backward: {{"type": "move", "direction": "backward", "speed": 0.5}}\n        - turn_left: {{"type": "turn", "direction": "left", "speed": 0.5}}\n        - turn_right: {{"type": "turn", "direction": "right", "speed": 0.5}}\n        - stop: {{"type": "stop"}}\n\n        Return only JSON."""\n\n        response = self.llm.generate_content(prompt)\n\n        try:\n            return json.loads(response.text)\n        except:\n            self.get_logger().error(f\'Failed to parse: {response.text}\')\n            return {"type": "stop"}\n\n    def execute_action(self, action):\n        twist = Twist()\n\n        if action["type"] == "move":\n            if action["direction"] == "forward":\n                twist.linear.x = action.get("speed", 0.5)\n            elif action["direction"] == "backward":\n                twist.linear.x = -action.get("speed", 0.5)\n\n        elif action["type"] == "turn":\n            if action["direction"] == "left":\n                twist.angular.z = action.get("speed", 0.5)\n            elif action["direction"] == "right":\n                twist.angular.z = -action.get("speed", 0.5)\n\n        elif action["type"] == "stop":\n            twist.linear.x = 0.0\n            twist.angular.z = 0.0\n\n        self.cmd_pub.publish(twist)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Executing: {action[\'type\']}"\n        self.status_pub.publish(status_msg)\n\ndef main():\n    rclpy.init()\n    node = VoiceControlNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"handling-ambiguity-and-safety",children:"Handling Ambiguity and Safety"}),"\n",(0,t.jsx)(n.h3,{id:"clarification-dialogs",children:"Clarification Dialogs"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class SafetyLayer:\n    def __init__(self):\n        self.dangerous_actions = ["drop", "throw", "hit"]\n        self.workspace_bounds = {\n            "x": (-1.0, 1.0),\n            "y": (-1.0, 1.0),\n            "z": (0.0, 1.0)\n        }\n\n    def validate_action(self, action):\n        # Check for dangerous keywords\n        action_str = json.dumps(action).lower()\n        for dangerous in self.dangerous_actions:\n            if dangerous in action_str:\n                return False, f"Action contains dangerous keyword: {dangerous}"\n\n        # Check workspace bounds\n        if "position" in action:\n            pos = action["position"]\n            for axis, (min_val, max_val) in self.workspace_bounds.items():\n                if axis in pos:\n                    if not (min_val <= pos[axis] <= max_val):\n                        return False, f"{axis} position out of bounds"\n\n        return True, "Action is safe"\n\n    def request_clarification(self, command, llm):\n        prompt = f"""The command "{command}" is ambiguous.\n        Generate a clarifying question for the user."""\n\n        response = llm.generate_content(prompt)\n        return response.text\n'})}),"\n",(0,t.jsx)(n.h3,{id:"multimodal-understanding",children:"Multimodal Understanding"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import cv2\nfrom PIL import Image\n\nclass MultimodalVLA:\n    def __init__(self):\n        self.llm = genai.GenerativeModel(\'gemini-2.0-flash-exp\')\n        self.transcriber = RealtimeTranscriber()\n\n    def process_multimodal_command(self, image_path):\n        # Get voice command\n        voice_command = self.transcriber.listen_and_transcribe()\n\n        # Load image\n        image = Image.open(image_path)\n\n        # Process with vision-language model\n        prompt = f"""User command: "{voice_command}"\n\n        Looking at the image, identify:\n        1. Objects mentioned in the command\n        2. Their locations\n        3. Feasibility of the action\n        4. Step-by-step plan\n\n        Return JSON plan."""\n\n        response = self.llm.generate_content([prompt, image])\n\n        return json.loads(response.text)\n\n# Usage\nvla = MultimodalVLA()\nplan = vla.process_multimodal_command("workspace.jpg")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Whisper provides robust, multilingual speech recognition"}),"\n",(0,t.jsx)(n.li,{children:"LLMs (Gemini, GPT-4) enable natural language understanding"}),"\n",(0,t.jsx)(n.li,{children:"Action grounding maps language to robot primitives"}),"\n",(0,t.jsx)(n.li,{children:"Safety layers validate commands before execution"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal VLA combines vision and language for context"}),"\n",(0,t.jsx)(n.li,{children:"Real-time transcription enables responsive voice control"}),"\n",(0,t.jsx)(n.li,{children:"Clarification dialogs handle ambiguous commands"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 integration enables deployment on real robots"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper"}),": ",(0,t.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"https://github.com/openai/whisper"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gemini API"}),": ",(0,t.jsx)(n.a,{href:"https://ai.google.dev/",children:"https://ai.google.dev/"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Research Papers"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Radford, A., et al. (2022). "Robust Speech Recognition via Large-Scale Weak Supervision" (Whisper)'}),"\n",(0,t.jsx)(n.li,{children:'Brohan, A., et al. (2023). "RT-2: Vision-Language-Action Models"'}),"\n",(0,t.jsx)(n.li,{children:'Driess, D., et al. (2023). "PaLM-E: An Embodied Multimodal Language Model"'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Libraries"}),":","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["OpenAI Whisper: ",(0,t.jsx)(n.a,{href:"https://pypi.org/project/openai-whisper/",children:"https://pypi.org/project/openai-whisper/"})]}),"\n",(0,t.jsxs)(n.li,{children:["Google Generative AI: ",(0,t.jsx)(n.a,{href:"https://pypi.org/project/google-generativeai/",children:"https://pypi.org/project/google-generativeai/"})]}),"\n"]}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);