"use strict";(globalThis.webpackChunkweb=globalThis.webpackChunkweb||[]).push([[6097],{7924:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"simulation/unity-hri","title":"02. Unity HRI","description":"Designing Human-Robot Interaction in Simulation","source":"@site/docs/simulation/unity-hri.md","sourceDirName":"simulation","slug":"/simulation/unity-hri","permalink":"/physical-ai-book/docs/simulation/unity-hri","draft":false,"unlisted":false,"editUrl":"https://github.com/AnasMehmood0/physical-ai-book/tree/main/web/docs/simulation/unity-hri.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"02. Unity HRI"},"sidebar":"tutorialSidebar","previous":{"title":"01. Gazebo Physics","permalink":"/physical-ai-book/docs/simulation/gazebo-physics"},"next":{"title":"01. Isaac SDK","permalink":"/physical-ai-book/docs/isaac/isaac-sdk"}}');var r=i(4848),o=i(8453);const a={sidebar_position:2,sidebar_label:"02. Unity HRI"},s="02. Unity HRI",l={},d=[{value:"Designing Human-Robot Interaction in Simulation",id:"designing-human-robot-interaction-in-simulation",level:2},{value:"Why Unity for Robotics Simulation?",id:"why-unity-for-robotics-simulation",level:2},{value:"Advantages of Unity",id:"advantages-of-unity",level:3},{value:"Unity vs. Traditional Robotics Simulators",id:"unity-vs-traditional-robotics-simulators",level:3},{value:"Unity ML-Agents Toolkit",id:"unity-ml-agents-toolkit",level:2},{value:"Architecture",id:"architecture",level:3},{value:"Installing ML-Agents",id:"installing-ml-agents",level:3},{value:"Creating a Simple Agent",id:"creating-a-simple-agent",level:3},{value:"Training Configuration",id:"training-configuration",level:3},{value:"Training the Agent",id:"training-the-agent",level:3},{value:"Creating Realistic HRI Scenarios",id:"creating-realistic-hri-scenarios",level:2},{value:"Virtual Human Avatars",id:"virtual-human-avatars",level:3},{value:"Gaze and Attention Tracking",id:"gaze-and-attention-tracking",level:3},{value:"Social Interaction Behaviors",id:"social-interaction-behaviors",level:3},{value:"Articulation Bodies for Robot Simulation",id:"articulation-bodies-for-robot-simulation",level:2},{value:"Why Articulation Bodies?",id:"why-articulation-bodies",level:3},{value:"Creating a Robot Arm",id:"creating-a-robot-arm",level:3},{value:"Gripper Control",id:"gripper-control",level:3},{value:"Sensor Simulation in Unity",id:"sensor-simulation-in-unity",level:2},{value:"Camera Sensors",id:"camera-sensors",level:3},{value:"Depth Camera",id:"depth-camera",level:3},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"ROS Integration with Unity",id:"ros-integration-with-unity",level:2},{value:"ROS-TCP-Connector",id:"ros-tcp-connector",level:3},{value:"Publishing from Unity to ROS 2",id:"publishing-from-unity-to-ros-2",level:3},{value:"Subscribing to ROS 2 in Unity",id:"subscribing-to-ros-2-in-unity",level:3},{value:"VR/AR for Robot Teleoperation",id:"vrar-for-robot-teleoperation",level:2},{value:"VR Teleoperation",id:"vr-teleoperation",level:3},{value:"AR Visualization",id:"ar-visualization",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Rendering Optimization",id:"rendering-optimization",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"02-unity-hri",children:"02. Unity HRI"})}),"\n",(0,r.jsx)(e.h2,{id:"designing-human-robot-interaction-in-simulation",children:"Designing Human-Robot Interaction in Simulation"}),"\n",(0,r.jsx)(e.p,{children:"This chapter focuses on designing and simulating human-robot interaction (HRI) within the Unity development environment. It covers the principles of intuitive HRI, including visual feedback, haptic interaction, and natural language processing in a simulated context. The chapter will guide readers through creating interactive 3D environments in Unity, developing virtual human avatars, and programming robot behaviors that respond to human input and gestures. Emphasis will be placed on evaluating user experience, iterating on interaction designs, and leveraging Unity's capabilities to build realistic and engaging HRI scenarios for research and development."}),"\n",(0,r.jsx)(e.h2,{id:"why-unity-for-robotics-simulation",children:"Why Unity for Robotics Simulation?"}),"\n",(0,r.jsx)(e.p,{children:"Unity is a powerful game engine that has become increasingly popular for robotics simulation, particularly for human-robot interaction scenarios."}),"\n",(0,r.jsx)(e.h3,{id:"advantages-of-unity",children:"Advantages of Unity"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"High-Quality Graphics:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Photorealistic rendering with HDRP (High Definition Render Pipeline)"}),"\n",(0,r.jsx)(e.li,{children:"Real-time lighting and shadows"}),"\n",(0,r.jsx)(e.li,{children:"Post-processing effects"}),"\n",(0,r.jsx)(e.li,{children:"Particle systems for environmental effects"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Cross-Platform Development:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Windows, macOS, Linux"}),"\n",(0,r.jsx)(e.li,{children:"VR/AR headsets (Meta Quest, HTC Vive, HoloLens)"}),"\n",(0,r.jsx)(e.li,{children:"Mobile devices (iOS, Android)"}),"\n",(0,r.jsx)(e.li,{children:"WebGL for browser-based simulations"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Rich Asset Ecosystem:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Unity Asset Store with thousands of 3D models, animations, and plugins"}),"\n",(0,r.jsx)(e.li,{children:"Humanoid avatars and animation systems"}),"\n",(0,r.jsx)(e.li,{children:"Environmental assets (buildings, furniture, outdoor scenes)"}),"\n",(0,r.jsx)(e.li,{children:"Prefabricated UI components"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Machine Learning Integration:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Unity ML-Agents for reinforcement learning"}),"\n",(0,r.jsx)(e.li,{children:"Barracuda inference engine for neural networks"}),"\n",(0,r.jsx)(e.li,{children:"Python API for custom training pipelines"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"User Interface and Interaction:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Intuitive UI system for control panels and feedback"}),"\n",(0,r.jsx)(e.li,{children:"Event system for interaction handling"}),"\n",(0,r.jsx)(e.li,{children:"Input system supporting keyboards, gamepads, VR controllers"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"unity-vs-traditional-robotics-simulators",children:"Unity vs. Traditional Robotics Simulators"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Feature"}),(0,r.jsx)(e.th,{children:"Unity"}),(0,r.jsx)(e.th,{children:"Gazebo/ROS-based"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Graphics"})}),(0,r.jsx)(e.td,{children:"Excellent"}),(0,r.jsx)(e.td,{children:"Good"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"HRI Focus"})}),(0,r.jsx)(e.td,{children:"Strong"}),(0,r.jsx)(e.td,{children:"Moderate"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Physics"})}),(0,r.jsx)(e.td,{children:"Good (PhysX)"}),(0,r.jsx)(e.td,{children:"Excellent"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"VR/AR"})}),(0,r.jsx)(e.td,{children:"Native support"}),(0,r.jsx)(e.td,{children:"Requires plugins"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Asset ecosystem"})}),(0,r.jsx)(e.td,{children:"Massive"}),(0,r.jsx)(e.td,{children:"Limited"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"ROS integration"})}),(0,r.jsx)(e.td,{children:"Via bridges"}),(0,r.jsx)(e.td,{children:"Native"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Learning curve"})}),(0,r.jsx)(e.td,{children:"Moderate"}),(0,r.jsx)(e.td,{children:"Steep"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"ML integration"})}),(0,r.jsx)(e.td,{children:"ML-Agents"}),(0,r.jsx)(e.td,{children:"Requires custom code"})]})]})]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"When to choose Unity:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"HRI research requiring realistic human avatars"}),"\n",(0,r.jsx)(e.li,{children:"VR/AR teleoperation interfaces"}),"\n",(0,r.jsx)(e.li,{children:"Consumer-facing demonstrations"}),"\n",(0,r.jsx)(e.li,{children:"Rapid prototyping of interaction scenarios"}),"\n",(0,r.jsx)(e.li,{children:"Training with visual diversity"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"When to choose Gazebo:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"High-fidelity physics simulation"}),"\n",(0,r.jsx)(e.li,{children:"Native ROS 2 integration"}),"\n",(0,r.jsx)(e.li,{children:"Multi-robot systems"}),"\n",(0,r.jsx)(e.li,{children:"Sensor accuracy is critical"}),"\n",(0,r.jsx)(e.li,{children:"Established ROS-based workflows"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"unity-ml-agents-toolkit",children:"Unity ML-Agents Toolkit"}),"\n",(0,r.jsx)(e.p,{children:"ML-Agents is Unity's framework for training intelligent agents using reinforcement learning, imitation learning, and neuroevolution."}),"\n",(0,r.jsx)(e.h3,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Unity Environment             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   Agent    \u2502\u25c4\u2500\u2500\u2500\u2500\u25ba\u2502 Environment  \u2502  \u2502\n\u2502  \u2502            \u2502      \u2502              \u2502  \u2502\n\u2502  \u2502 - Brain    \u2502      \u2502 - Rewards    \u2502  \u2502\n\u2502  \u2502 - Sensors  \u2502      \u2502 - Reset      \u2502  \u2502\n\u2502  \u2502 - Actions  \u2502      \u2502              \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n            \u25b2                   \u25bc\n            \u2502  Python API (Gym Interface)\n            \u25bc                   \u25b2\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       Python Training Process           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   PPO/SAC  \u2502\u25c4\u2500\u2500\u2500\u2500\u25ba\u2502  TensorFlow  \u2502  \u2502\n\u2502  \u2502   Trainer  \u2502      \u2502   /PyTorch   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(e.h3,{id:"installing-ml-agents",children:"Installing ML-Agents"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Install Python package\npip install mlagents\n\n# Install Unity package (in Unity Editor)\n# Window \u2192 Package Manager \u2192 Add package from git URL:\n# com.unity.ml-agents\n"})}),"\n",(0,r.jsx)(e.h3,{id:"creating-a-simple-agent",children:"Creating a Simple Agent"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"C# Agent Script:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.MLAgents;\nusing Unity.MLAgents.Sensors;\nusing Unity.MLAgents.Actuators;\n\npublic class RobotAgent : Agent\n{\n    [SerializeField] private Transform target;\n    [SerializeField] private float moveSpeed = 5f;\n    [SerializeField] private float rotationSpeed = 100f;\n\n    private Rigidbody rb;\n\n    public override void Initialize()\n    {\n        rb = GetComponent<Rigidbody>();\n    }\n\n    public override void OnEpisodeBegin()\n    {\n        // Reset agent position\n        transform.localPosition = new Vector3(\n            Random.Range(-4f, 4f),\n            0.5f,\n            Random.Range(-4f, 4f)\n        );\n        rb.velocity = Vector3.zero;\n        rb.angularVelocity = Vector3.zero;\n\n        // Reset target position\n        target.localPosition = new Vector3(\n            Random.Range(-4f, 4f),\n            0.5f,\n            Random.Range(-4f, 4f)\n        );\n    }\n\n    public override void CollectObservations(VectorSensor sensor)\n    {\n        // Agent position (3 values)\n        sensor.AddObservation(transform.localPosition);\n\n        // Target position (3 values)\n        sensor.AddObservation(target.localPosition);\n\n        // Agent velocity (3 values)\n        sensor.AddObservation(rb.velocity);\n\n        // Direction to target (3 values)\n        Vector3 directionToTarget = (target.localPosition - transform.localPosition).normalized;\n        sensor.AddObservation(directionToTarget);\n\n        // Distance to target (1 value)\n        float distance = Vector3.Distance(transform.localPosition, target.localPosition);\n        sensor.AddObservation(distance);\n\n        // Total: 13 observations\n    }\n\n    public override void OnActionReceived(ActionBuffers actions)\n    {\n        // Actions: [0] = forward/backward, [1] = rotation\n        float moveZ = actions.ContinuousActions[0];\n        float rotation = actions.ContinuousActions[1];\n\n        // Apply movement\n        Vector3 movement = transform.forward * moveZ * moveSpeed * Time.fixedDeltaTime;\n        rb.MovePosition(rb.position + movement);\n\n        // Apply rotation\n        transform.Rotate(0f, rotation * rotationSpeed * Time.fixedDeltaTime, 0f);\n\n        // Reward shaping\n        float distanceToTarget = Vector3.Distance(transform.localPosition, target.localPosition);\n\n        // Small negative reward for time (encourage speed)\n        AddReward(-0.001f);\n\n        // Reward for getting closer\n        AddReward(-distanceToTarget * 0.01f);\n\n        // Check if reached target\n        if (distanceToTarget < 1.5f)\n        {\n            AddReward(1.0f);\n            EndEpisode();\n        }\n\n        // Fall off platform\n        if (transform.localPosition.y < 0)\n        {\n            AddReward(-1.0f);\n            EndEpisode();\n        }\n    }\n\n    public override void Heuristic(in ActionBuffers actionsOut)\n    {\n        // Manual control for testing\n        var continuousActions = actionsOut.ContinuousActions;\n        continuousActions[0] = Input.GetAxis("Vertical");\n        continuousActions[1] = Input.GetAxis("Horizontal");\n    }\n\n    private void OnCollisionEnter(Collision collision)\n    {\n        if (collision.gameObject.CompareTag("Target"))\n        {\n            AddReward(1.0f);\n            EndEpisode();\n        }\n        else if (collision.gameObject.CompareTag("Obstacle"))\n        {\n            AddReward(-0.5f);\n        }\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"training-configuration",children:"Training Configuration"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"trainer_config.yaml:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-yaml",children:"behaviors:\n  RobotAgent:\n    trainer_type: ppo\n    hyperparameters:\n      batch_size: 1024\n      buffer_size: 10240\n      learning_rate: 0.0003\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n      learning_rate_schedule: linear\n    network_settings:\n      normalize: true\n      hidden_units: 256\n      num_layers: 2\n      vis_encode_type: simple\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n    max_steps: 500000\n    time_horizon: 64\n    summary_freq: 10000\n"})}),"\n",(0,r.jsx)(e.h3,{id:"training-the-agent",children:"Training the Agent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# Start training\nmlagents-learn trainer_config.yaml --run-id=robot_navigation_01\n\n# Resume training\nmlagents-learn trainer_config.yaml --run-id=robot_navigation_01 --resume\n\n# Monitor with TensorBoard\ntensorboard --logdir results/\n"})}),"\n",(0,r.jsx)(e.h2,{id:"creating-realistic-hri-scenarios",children:"Creating Realistic HRI Scenarios"}),"\n",(0,r.jsx)(e.h3,{id:"virtual-human-avatars",children:"Virtual Human Avatars"}),"\n",(0,r.jsx)(e.p,{children:"Unity's Humanoid rig system allows for realistic human animations and interaction."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Setting Up Human Avatar:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.AI;\n\npublic class VirtualHuman : MonoBehaviour\n{\n    [SerializeField] private Animator animator;\n    [SerializeField] private NavMeshAgent navAgent;\n    [SerializeField] private Transform robot;\n\n    private bool isInteracting = false;\n    private float interactionDistance = 2f;\n\n    void Update()\n    {\n        // Calculate distance to robot\n        float distance = Vector3.Distance(transform.position, robot.position);\n\n        // Update animation based on state\n        if (distance < interactionDistance && !isInteracting)\n        {\n            StartInteraction();\n        }\n        else if (distance >= interactionDistance && isInteracting)\n        {\n            EndInteraction();\n        }\n\n        // Update animator parameters\n        animator.SetFloat("Speed", navAgent.velocity.magnitude);\n        animator.SetBool("IsInteracting", isInteracting);\n    }\n\n    private void StartInteraction()\n    {\n        isInteracting = true;\n        navAgent.isStopped = true;\n\n        // Look at robot\n        Vector3 lookDirection = robot.position - transform.position;\n        lookDirection.y = 0;\n        transform.rotation = Quaternion.LookRotation(lookDirection);\n\n        // Trigger wave animation\n        animator.SetTrigger("Wave");\n\n        Debug.Log("Human started interaction with robot");\n    }\n\n    private void EndInteraction()\n    {\n        isInteracting = false;\n        navAgent.isStopped = false;\n        animator.SetTrigger("Walk");\n    }\n\n    public void ReceiveObjectFromRobot(GameObject obj)\n    {\n        // Play receive animation\n        animator.SetTrigger("Receive");\n\n        // Hand IK to position hands correctly\n        StartCoroutine(PerformHandoff(obj));\n    }\n\n    private System.Collections.IEnumerator PerformHandoff(GameObject obj)\n    {\n        // Enable IK\n        animator.SetIKPositionWeight(AvatarIKGoal.RightHand, 1f);\n        animator.SetIKRotationWeight(AvatarIKGoal.RightHand, 1f);\n\n        float duration = 1f;\n        float elapsed = 0f;\n\n        Transform rightHand = animator.GetBoneTransform(HumanBodyBones.RightHand);\n\n        while (elapsed < duration)\n        {\n            // Move hand to object\n            animator.SetIKPosition(AvatarIKGoal.RightHand, obj.transform.position);\n            animator.SetIKRotation(AvatarIKGoal.RightHand, obj.transform.rotation);\n\n            elapsed += Time.deltaTime;\n            yield return null;\n        }\n\n        // Attach object to hand\n        obj.transform.SetParent(rightHand);\n        obj.transform.localPosition = Vector3.zero;\n\n        yield return new WaitForSeconds(0.5f);\n\n        // Disable IK\n        animator.SetIKPositionWeight(AvatarIKGoal.RightHand, 0f);\n        animator.SetIKRotationWeight(AvatarIKGoal.RightHand, 0f);\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"gaze-and-attention-tracking",children:"Gaze and Attention Tracking"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class GazeController : MonoBehaviour\n{\n    [SerializeField] private Animator animator;\n    [SerializeField] private Transform eyeTarget;\n    [SerializeField] private float gazeSpeed = 2f;\n    [SerializeField] private float blinkInterval = 3f;\n\n    private Transform currentLookTarget;\n    private float nextBlinkTime;\n\n    void Start()\n    {\n        nextBlinkTime = Time.time + blinkInterval;\n    }\n\n    void OnAnimatorIK(int layerIndex)\n    {\n        if (currentLookTarget != null)\n        {\n            // Look at target\n            animator.SetLookAtWeight(1f, 0.5f, 1f, 0f, 0.5f);\n            animator.SetLookAtPosition(currentLookTarget.position);\n        }\n        else\n        {\n            animator.SetLookAtWeight(0f);\n        }\n\n        // Blink periodically\n        if (Time.time > nextBlinkTime)\n        {\n            animator.SetTrigger("Blink");\n            nextBlinkTime = Time.time + Random.Range(2f, 5f);\n        }\n    }\n\n    public void SetGazeTarget(Transform target)\n    {\n        currentLookTarget = target;\n    }\n\n    public void ClearGazeTarget()\n    {\n        currentLookTarget = null;\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"social-interaction-behaviors",children:"Social Interaction Behaviors"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class SocialBehaviorController : MonoBehaviour\n{\n    public enum Emotion\n    {\n        Neutral,\n        Happy,\n        Surprised,\n        Confused,\n        Frustrated\n    }\n\n    [SerializeField] private Animator animator;\n    [SerializeField] private AudioSource voiceSource;\n    [SerializeField] private Dictionary<Emotion, AudioClip> emotionSounds;\n\n    private Emotion currentEmotion = Emotion.Neutral;\n\n    public void ExpressEmotion(Emotion emotion)\n    {\n        currentEmotion = emotion;\n\n        switch (emotion)\n        {\n            case Emotion.Happy:\n                animator.SetTrigger("Smile");\n                animator.SetFloat("EmotionBlend", 1f);\n                break;\n            case Emotion.Surprised:\n                animator.SetTrigger("Surprise");\n                animator.SetFloat("EmotionBlend", 0.5f);\n                break;\n            case Emotion.Confused:\n                animator.SetTrigger("TiltHead");\n                animator.SetFloat("EmotionBlend", -0.5f);\n                break;\n            case Emotion.Frustrated:\n                animator.SetTrigger("CrossArms");\n                animator.SetFloat("EmotionBlend", -1f);\n                break;\n            default:\n                animator.SetFloat("EmotionBlend", 0f);\n                break;\n        }\n\n        // Play corresponding sound\n        if (emotionSounds.ContainsKey(emotion))\n        {\n            voiceSource.PlayOneShot(emotionSounds[emotion]);\n        }\n    }\n\n    public void PointAt(Vector3 worldPosition)\n    {\n        // Calculate pointing direction\n        Vector3 direction = worldPosition - transform.position;\n        direction.y = 0;\n\n        // Orient body towards target\n        transform.rotation = Quaternion.LookRotation(direction);\n\n        // Trigger pointing animation\n        animator.SetTrigger("Point");\n\n        // Use IK to point accurately\n        StartCoroutine(PointingIK(worldPosition));\n    }\n\n    private System.Collections.IEnumerator PointingIK(Vector3 target)\n    {\n        float duration = 2f;\n        float elapsed = 0f;\n\n        while (elapsed < duration)\n        {\n            animator.SetIKPositionWeight(AvatarIKGoal.RightHand, 1f);\n            animator.SetIKPosition(AvatarIKGoal.RightHand, target);\n\n            elapsed += Time.deltaTime;\n            yield return null;\n        }\n\n        animator.SetIKPositionWeight(AvatarIKGoal.RightHand, 0f);\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"articulation-bodies-for-robot-simulation",children:"Articulation Bodies for Robot Simulation"}),"\n",(0,r.jsx)(e.p,{children:"Unity's Articulation Body system provides high-fidelity physics simulation for robotic systems."}),"\n",(0,r.jsx)(e.h3,{id:"why-articulation-bodies",children:"Why Articulation Bodies?"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Advantages over Rigidbody + Joints:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"More stable for complex kinematic chains"}),"\n",(0,r.jsx)(e.li,{children:"Reduced jitter and drift"}),"\n",(0,r.jsx)(e.li,{children:"Better performance for robots with many joints"}),"\n",(0,r.jsx)(e.li,{children:"Accurate force/torque control"}),"\n",(0,r.jsx)(e.li,{children:"Built-in friction and damping models"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"creating-a-robot-arm",children:"Creating a Robot Arm"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class RobotArmController : MonoBehaviour\n{\n    [SerializeField] private ArticulationBody[] joints;\n    [SerializeField] private float maxVelocity = 1f;\n    [SerializeField] private float maxForce = 100f;\n\n    private float[] targetPositions;\n    private float[] targetVelocities;\n\n    void Start()\n    {\n        targetPositions = new float[joints.Length];\n        targetVelocities = new float[joints.Length];\n\n        // Configure each joint\n        for (int i = 0; i < joints.Length; i++)\n        {\n            ConfigureJoint(joints[i]);\n        }\n    }\n\n    void ConfigureJoint(ArticulationBody joint)\n    {\n        // Set joint properties\n        joint.jointFriction = 0.05f;\n        joint.angularDamping = 0.5f;\n        joint.linearDamping = 0.5f;\n\n        // Configure drive\n        var drive = joint.xDrive;\n        drive.stiffness = 1000f;\n        drive.damping = 100f;\n        drive.forceLimit = maxForce;\n        joint.xDrive = drive;\n    }\n\n    public void SetJointTarget(int jointIndex, float targetAngle)\n    {\n        if (jointIndex >= 0 && jointIndex < joints.Length)\n        {\n            var drive = joints[jointIndex].xDrive;\n            drive.target = targetAngle * Mathf.Rad2Deg; // Convert to degrees\n            joints[jointIndex].xDrive = drive;\n        }\n    }\n\n    public void SetJointVelocity(int jointIndex, float velocity)\n    {\n        if (jointIndex >= 0 && jointIndex < joints.Length)\n        {\n            var drive = joints[jointIndex].xDrive;\n            drive.targetVelocity = velocity * Mathf.Rad2Deg;\n            joints[jointIndex].xDrive = drive;\n        }\n    }\n\n    public float GetJointPosition(int jointIndex)\n    {\n        if (jointIndex >= 0 && jointIndex < joints.Length)\n        {\n            return joints[jointIndex].jointPosition[0] * Mathf.Deg2Rad;\n        }\n        return 0f;\n    }\n\n    public float GetJointVelocity(int jointIndex)\n    {\n        if (jointIndex >= 0 && jointIndex < joints.Length)\n        {\n            return joints[jointIndex].jointVelocity[0] * Mathf.Deg2Rad;\n        }\n        return 0f;\n    }\n\n    public void MoveToConfiguration(float[] angles)\n    {\n        for (int i = 0; i < Mathf.Min(angles.Length, joints.Length); i++)\n        {\n            SetJointTarget(i, angles[i]);\n        }\n    }\n\n    // Forward kinematics\n    public Vector3 GetEndEffectorPosition()\n    {\n        if (joints.Length > 0)\n        {\n            ArticulationBody lastJoint = joints[joints.Length - 1];\n            return lastJoint.transform.position;\n        }\n        return Vector3.zero;\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h3,{id:"gripper-control",children:"Gripper Control"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class RobotGripper : MonoBehaviour\n{\n    [SerializeField] private ArticulationBody leftFinger;\n    [SerializeField] private ArticulationBody rightFinger;\n    [SerializeField] private float gripForce = 50f;\n    [SerializeField] private float openAngle = 0f;\n    [SerializeField] private float closeAngle = 45f;\n\n    private bool isGrasping = false;\n    private List<Rigidbody> graspedObjects = new List<Rigidbody>();\n\n    public void Open()\n    {\n        SetFingerAngle(leftFinger, openAngle);\n        SetFingerAngle(rightFinger, openAngle);\n        isGrasping = false;\n\n        // Release grasped objects\n        foreach (var obj in graspedObjects)\n        {\n            if (obj != null)\n            {\n                obj.isKinematic = false;\n                obj.transform.SetParent(null);\n            }\n        }\n        graspedObjects.Clear();\n    }\n\n    public void Close()\n    {\n        SetFingerAngle(leftFinger, closeAngle);\n        SetFingerAngle(rightFinger, closeAngle);\n        isGrasping = true;\n\n        // Check for objects to grasp\n        CheckGrasp();\n    }\n\n    private void SetFingerAngle(ArticulationBody finger, float angle)\n    {\n        var drive = finger.xDrive;\n        drive.target = angle;\n        drive.forceLimit = gripForce;\n        finger.xDrive = drive;\n    }\n\n    private void CheckGrasp()\n    {\n        // Find objects between fingers\n        Vector3 center = (leftFinger.transform.position + rightFinger.transform.position) / 2f;\n        float distance = Vector3.Distance(leftFinger.transform.position, rightFinger.transform.position);\n\n        Collider[] colliders = Physics.OverlapSphere(center, distance / 2f);\n\n        foreach (var collider in colliders)\n        {\n            if (collider.CompareTag("Graspable"))\n            {\n                Rigidbody rb = collider.GetComponent<Rigidbody>();\n                if (rb != null && !graspedObjects.Contains(rb))\n                {\n                    // Attach object\n                    rb.isKinematic = true;\n                    rb.transform.SetParent(transform);\n                    graspedObjects.Add(rb);\n                }\n            }\n        }\n    }\n\n    public bool IsGrasping()\n    {\n        return isGrasping && graspedObjects.Count > 0;\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"sensor-simulation-in-unity",children:"Sensor Simulation in Unity"}),"\n",(0,r.jsx)(e.h3,{id:"camera-sensors",children:"Camera Sensors"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class RobotCamera : MonoBehaviour\n{\n    [SerializeField] private Camera sensorCamera;\n    [SerializeField] private int width = 640;\n    [SerializeField] private int height = 480;\n    [SerializeField] private float updateRate = 30f;\n\n    private RenderTexture renderTexture;\n    private Texture2D outputTexture;\n    private float nextUpdateTime;\n\n    void Start()\n    {\n        // Create render texture\n        renderTexture = new RenderTexture(width, height, 24);\n        sensorCamera.targetTexture = renderTexture;\n\n        outputTexture = new Texture2D(width, height, TextureFormat.RGB24, false);\n    }\n\n    void Update()\n    {\n        if (Time.time >= nextUpdateTime)\n        {\n            CaptureImage();\n            nextUpdateTime = Time.time + (1f / updateRate);\n        }\n    }\n\n    void CaptureImage()\n    {\n        // Render camera\n        sensorCamera.Render();\n\n        // Read pixels\n        RenderTexture.active = renderTexture;\n        outputTexture.ReadPixels(new Rect(0, 0, width, height), 0, 0);\n        outputTexture.Apply();\n        RenderTexture.active = null;\n\n        // Process or send to ML system\n        ProcessImage(outputTexture);\n    }\n\n    void ProcessImage(Texture2D image)\n    {\n        // Example: Send to Python for processing\n        byte[] imageBytes = image.EncodeToJPG();\n        // SendToPython(imageBytes);\n    }\n\n    public Texture2D GetLatestImage()\n    {\n        return outputTexture;\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h3,{id:"depth-camera",children:"Depth Camera"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\n[RequireComponent(typeof(Camera))]\npublic class DepthCamera : MonoBehaviour\n{\n    [SerializeField] private Shader depthShader;\n    [SerializeField] private float maxDistance = 10f;\n\n    private Camera depthCamera;\n    private RenderTexture depthTexture;\n\n    void Start()\n    {\n        depthCamera = GetComponent<Camera>();\n        depthCamera.depthTextureMode = DepthTextureMode.Depth;\n\n        depthTexture = new RenderTexture(640, 480, 24, RenderTextureFormat.RFloat);\n        depthCamera.targetTexture = depthTexture;\n\n        if (depthShader != null)\n        {\n            depthCamera.SetReplacementShader(depthShader, "RenderType");\n        }\n    }\n\n    public float[,] GetDepthData()\n    {\n        RenderTexture.active = depthTexture;\n        Texture2D tex = new Texture2D(depthTexture.width, depthTexture.height, TextureFormat.RFloat, false);\n        tex.ReadPixels(new Rect(0, 0, depthTexture.width, depthTexture.height), 0, 0);\n        tex.Apply();\n\n        float[,] depthData = new float[depthTexture.width, depthTexture.height];\n\n        for (int y = 0; y < depthTexture.height; y++)\n        {\n            for (int x = 0; x < depthTexture.width; x++)\n            {\n                float depth = tex.GetPixel(x, y).r;\n                depthData[x, y] = depth * maxDistance;\n            }\n        }\n\n        RenderTexture.active = null;\n        Destroy(tex);\n\n        return depthData;\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing System.Collections.Generic;\n\npublic class LidarSensor : MonoBehaviour\n{\n    [SerializeField] private int horizontalRays = 360;\n    [SerializeField] private int verticalRays = 16;\n    [SerializeField] private float horizontalFOV = 360f;\n    [SerializeField] private float verticalFOV = 30f;\n    [SerializeField] private float maxRange = 100f;\n    [SerializeField] private LayerMask detectionMask;\n    [SerializeField] private float scanRate = 10f;\n\n    private List<Vector3> pointCloud = new List<Vector3>();\n    private float nextScanTime;\n\n    void Update()\n    {\n        if (Time.time >= nextScanTime)\n        {\n            PerformScan();\n            nextScanTime = Time.time + (1f / scanRate);\n        }\n    }\n\n    void PerformScan()\n    {\n        pointCloud.Clear();\n\n        float horizontalStep = horizontalFOV / horizontalRays;\n        float verticalStep = verticalFOV / (verticalRays - 1);\n        float verticalStart = -verticalFOV / 2f;\n\n        for (int v = 0; v < verticalRays; v++)\n        {\n            float verticalAngle = verticalStart + (v * verticalStep);\n\n            for (int h = 0; h < horizontalRays; h++)\n            {\n                float horizontalAngle = h * horizontalStep;\n\n                // Calculate ray direction\n                Vector3 direction = Quaternion.Euler(verticalAngle, horizontalAngle, 0) * transform.forward;\n\n                // Cast ray\n                if (Physics.Raycast(transform.position, transform.TransformDirection(direction),\n                    out RaycastHit hit, maxRange, detectionMask))\n                {\n                    pointCloud.Add(hit.point);\n                }\n            }\n        }\n    }\n\n    public List<Vector3> GetPointCloud()\n    {\n        return new List<Vector3>(pointCloud);\n    }\n\n    void OnDrawGizmos()\n    {\n        if (pointCloud != null && pointCloud.Count > 0)\n        {\n            Gizmos.color = Color.red;\n            foreach (var point in pointCloud)\n            {\n                Gizmos.DrawSphere(point, 0.05f);\n            }\n        }\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"ros-integration-with-unity",children:"ROS Integration with Unity"}),"\n",(0,r.jsx)(e.h3,{id:"ros-tcp-connector",children:"ROS-TCP-Connector"}),"\n",(0,r.jsx)(e.p,{children:"Unity's ROS-TCP-Connector enables communication between Unity and ROS 2."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Installation:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-bash",children:"# In Unity: Package Manager \u2192 Add from git URL\nhttps://github.com/Unity-Technologies/ROS-TCP-Connector.git?path=/com.unity.robotics.ros-tcp-connector\n\n# In ROS 2 workspace\ncd ~/ros2_ws/src\ngit clone https://github.com/Unity-Technologies/ROS-TCP-Endpoint\ncd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n\n# Launch ROS-TCP endpoint\nros2 run ros_tcp_endpoint default_server_endpoint --ros-args -p ROS_IP:=127.0.0.1\n"})}),"\n",(0,r.jsx)(e.h3,{id:"publishing-from-unity-to-ros-2",children:"Publishing from Unity to ROS 2"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\n\npublic class UnityToROS : MonoBehaviour\n{\n    [SerializeField] private string topicName = "/unity/robot_pose";\n    [SerializeField] private float publishRate = 10f;\n\n    private ROSConnection ros;\n    private float nextPublishTime;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<PoseStampedMsg>(topicName);\n    }\n\n    void Update()\n    {\n        if (Time.time >= nextPublishTime)\n        {\n            PublishPose();\n            nextPublishTime = Time.time + (1f / publishRate);\n        }\n    }\n\n    void PublishPose()\n    {\n        var poseMsg = new PoseStampedMsg();\n\n        // Header\n        poseMsg.header.stamp = new TimeStamp(Clock.time);\n        poseMsg.header.frame_id = "unity_world";\n\n        // Position\n        poseMsg.pose.position = new PointMsg\n        {\n            x = transform.position.x,\n            y = transform.position.y,\n            z = transform.position.z\n        };\n\n        // Orientation\n        poseMsg.pose.orientation = new QuaternionMsg\n        {\n            x = transform.rotation.x,\n            y = transform.rotation.y,\n            z = transform.rotation.z,\n            w = transform.rotation.w\n        };\n\n        ros.Publish(topicName, poseMsg);\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"subscribing-to-ros-2-in-unity",children:"Subscribing to ROS 2 in Unity"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Geometry;\n\npublic class ROSToUnity : MonoBehaviour\n{\n    [SerializeField] private string topicName = "/cmd_vel";\n    [SerializeField] private float moveSpeed = 5f;\n    [SerializeField] private float rotationSpeed = 100f;\n\n    private ROSConnection ros;\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.Subscribe<TwistMsg>(topicName, ReceiveVelocityCommand);\n    }\n\n    void ReceiveVelocityCommand(TwistMsg msg)\n    {\n        // Apply velocity command\n        float linear = (float)msg.linear.x;\n        float angular = (float)msg.angular.z;\n\n        Vector3 movement = transform.forward * linear * moveSpeed * Time.deltaTime;\n        transform.position += movement;\n\n        transform.Rotate(0, angular * rotationSpeed * Time.deltaTime, 0);\n    }\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"vrar-for-robot-teleoperation",children:"VR/AR for Robot Teleoperation"}),"\n",(0,r.jsx)(e.h3,{id:"vr-teleoperation",children:"VR Teleoperation"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing UnityEngine.XR;\nusing UnityEngine.XR.Interaction.Toolkit;\n\npublic class VRRobotController : MonoBehaviour\n{\n    [SerializeField] private Transform vrLeftHand;\n    [SerializeField] private Transform vrRightHand;\n    [SerializeField] private RobotArmController leftArm;\n    [SerializeField] private RobotArmController rightArm;\n    [SerializeField] private float scaleFactor = 1f;\n\n    private Vector3 leftHandInitialPos;\n    private Vector3 rightHandInitialPos;\n\n    void Start()\n    {\n        leftHandInitialPos = vrLeftHand.position;\n        rightHandInitialPos = vrRightHand.position;\n    }\n\n    void Update()\n    {\n        // Map VR hand positions to robot arms\n        Vector3 leftHandDelta = (vrLeftHand.position - leftHandInitialPos) * scaleFactor;\n        Vector3 rightHandDelta = (vrRightHand.position - rightHandInitialPos) * scaleFactor;\n\n        // Inverse kinematics would go here\n        // For simplicity, direct mapping:\n        if (leftArm != null)\n        {\n            MoveArmToTarget(leftArm, leftHandDelta);\n        }\n\n        if (rightArm != null)\n        {\n            MoveArmToTarget(rightArm, rightHandDelta);\n        }\n\n        // Gripper control via trigger buttons\n        if (GetTriggerPressed(XRNode.LeftHand))\n        {\n            leftArm.GetComponent<RobotGripper>()?.Close();\n        }\n        else\n        {\n            leftArm.GetComponent<RobotGripper>()?.Open();\n        }\n    }\n\n    void MoveArmToTarget(RobotArmController arm, Vector3 targetDelta)\n    {\n        // Simple IK approximation\n        // In production, use a proper IK solver\n        Vector3 targetPos = arm.GetEndEffectorPosition() + targetDelta;\n\n        // Calculate joint angles (simplified)\n        // Real implementation would use numerical IK\n    }\n\n    bool GetTriggerPressed(XRNode hand)\n    {\n        InputDevice device = InputDevices.GetDeviceAtXRNode(hand);\n        if (device.TryGetFeatureValue(CommonUsages.trigger, out float value))\n        {\n            return value > 0.5f;\n        }\n        return false;\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h3,{id:"ar-visualization",children:"AR Visualization"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\nusing UnityEngine.XR.ARFoundation;\n\npublic class ARRobotVisualizer : MonoBehaviour\n{\n    [SerializeField] private ARRaycastManager raycastManager;\n    [SerializeField] private GameObject robotPrefab;\n\n    private GameObject spawnedRobot;\n    private List<ARRaycastHit> hits = new List<ARRaycastHit>();\n\n    void Update()\n    {\n        // Place robot on tap\n        if (Input.touchCount > 0)\n        {\n            Touch touch = Input.GetTouch(0);\n\n            if (touch.phase == TouchPhase.Began)\n            {\n                if (raycastManager.Raycast(touch.position, hits, TrackableType.Planes))\n                {\n                    Pose hitPose = hits[0].pose;\n\n                    if (spawnedRobot == null)\n                    {\n                        spawnedRobot = Instantiate(robotPrefab, hitPose.position, hitPose.rotation);\n                    }\n                    else\n                    {\n                        spawnedRobot.transform.position = hitPose.position;\n                    }\n                }\n            }\n        }\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,r.jsx)(e.h3,{id:"rendering-optimization",children:"Rendering Optimization"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\n\npublic class PerformanceManager : MonoBehaviour\n{\n    [SerializeField] private bool useLOD = true;\n    [SerializeField] private bool useOcclusion Culling = true;\n    [SerializeField] private int targetFrameRate = 60;\n\n    void Start()\n    {\n        // Set target frame rate\n        Application.targetFrameRate = targetFrameRate;\n\n        // Quality settings\n        QualitySettings.vSyncCount = 0;\n\n        // Optimize physics\n        Physics.defaultSolverIterations = 6;\n        Physics.defaultSolverVelocityIterations = 1;\n\n        // Time scale for faster simulation\n        // Time.timeScale = 2.0f; // 2x speed\n    }\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Unity excels at HRI scenarios with rich graphics, VR/AR, and human avatars"}),"\n",(0,r.jsx)(e.li,{children:"ML-Agents provides integrated reinforcement learning for robot training"}),"\n",(0,r.jsx)(e.li,{children:"Articulation Bodies offer stable, high-fidelity robot joint simulation"}),"\n",(0,r.jsx)(e.li,{children:"ROS-TCP-Connector enables bidirectional communication with ROS 2"}),"\n",(0,r.jsx)(e.li,{children:"VR/AR capabilities allow immersive teleoperation and visualization"}),"\n",(0,r.jsx)(e.li,{children:"Sensor simulation (cameras, depth, LiDAR) provides realistic perception data"}),"\n",(0,r.jsx)(e.li,{children:"Unity's asset ecosystem accelerates development of interactive scenarios"}),"\n",(0,r.jsx)(e.li,{children:"Performance optimization is crucial for real-time simulation and training"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity ML-Agents"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/Unity-Technologies/ml-agents",children:"ML-Agents Documentation"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Unity Robotics Hub"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/Unity-Technologies/Unity-Robotics-Hub",children:"Unity Robotics"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS Integration"}),": ",(0,r.jsx)(e.a,{href:"https://github.com/Unity-Technologies/ROS-TCP-Connector",children:"ROS-TCP-Connector"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Articulation Bodies"}),": ",(0,r.jsx)(e.a,{href:"https://docs.unity3d.com/Manual/physics-articulations.html",children:"Unity Physics Documentation"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Books"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'"Unity Artificial Intelligence Programming" by Dr. Davide Aversa'}),"\n",(0,r.jsx)(e.li,{children:'"Hands-On Unity 2021 Game Development" by Nicolas Alejandro Borromeo'}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Research Papers"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'Juliani, A., et al. (2018). "Unity: A General Platform for Intelligent Agents"'}),"\n",(0,r.jsx)(e.li,{children:'Todorov, E., et al. (2012). "MuJoCo: A physics engine for model-based control" (comparison context)'}),"\n"]}),"\n"]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const r={},o=t.createContext(r);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);