"use strict";(globalThis.webpackChunkweb=globalThis.webpackChunkweb||[]).push([[9470],{4293:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"isaac/reinforcement-learning","title":"02. Reinforcement Learning","description":"Training Agents for Complex Robotic Tasks","source":"@site/docs/isaac/reinforcement-learning.md","sourceDirName":"isaac","slug":"/isaac/reinforcement-learning","permalink":"/physical-ai-book/docs/isaac/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/AnasMehmood0/physical-ai-book/tree/main/web/docs/isaac/reinforcement-learning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"sidebar_label":"02. Reinforcement Learning"},"sidebar":"tutorialSidebar","previous":{"title":"01. Isaac SDK","permalink":"/physical-ai-book/docs/isaac/isaac-sdk"},"next":{"title":"01. Whisper Voice","permalink":"/physical-ai-book/docs/vla/whisper-voice"}}');var s=t(4848),r=t(8453);const i={sidebar_position:2,sidebar_label:"02. Reinforcement Learning"},o="02. Reinforcement Learning",l={},c=[{value:"Training Agents for Complex Robotic Tasks",id:"training-agents-for-complex-robotic-tasks",level:2},{value:"Fundamentals of Reinforcement Learning",id:"fundamentals-of-reinforcement-learning",level:2},{value:"Markov Decision Process (MDP)",id:"markov-decision-process-mdp",level:3},{value:"Key Concepts",id:"key-concepts",level:3},{value:"RL Algorithms for Robotics",id:"rl-algorithms-for-robotics",level:2},{value:"Q-Learning (Off-Policy)",id:"q-learning-off-policy",level:3},{value:"Deep Q-Network (DQN)",id:"deep-q-network-dqn",level:3},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Soft Actor-Critic (SAC)",id:"soft-actor-critic-sac",level:3},{value:"Reward Shaping for Robotics",id:"reward-shaping-for-robotics",level:2},{value:"Manipulation Tasks",id:"manipulation-tasks",level:3},{value:"Locomotion Tasks",id:"locomotion-tasks",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Residual RL",id:"residual-rl",level:3},{value:"Training with Isaac Gym",id:"training-with-isaac-gym",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"02-reinforcement-learning",children:"02. Reinforcement Learning"})}),"\n",(0,s.jsx)(e.h2,{id:"training-agents-for-complex-robotic-tasks",children:"Training Agents for Complex Robotic Tasks"}),"\n",(0,s.jsx)(e.p,{children:"This chapter provides an in-depth exploration of Reinforcement Learning (RL) techniques specifically tailored for training agents to perform complex robotic tasks. It covers fundamental RL concepts such as states, actions, rewards, and policies, and introduces key algorithms like Q-learning, SARSA, Policy Gradients (REINFORCE), and Actor-Critic methods (A2C/A3C)."}),"\n",(0,s.jsx)(e.h2,{id:"fundamentals-of-reinforcement-learning",children:"Fundamentals of Reinforcement Learning"}),"\n",(0,s.jsx)(e.h3,{id:"markov-decision-process-mdp",children:"Markov Decision Process (MDP)"}),"\n",(0,s.jsx)(e.p,{children:"An MDP is defined by the tuple (S, A, P, R, \u03b3):"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"S"}),": State space"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"A"}),": Action space"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"P"}),": Transition probability P(s'|s,a)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"R"}),": Reward function R(s,a,s')"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"\u03b3"}),": Discount factor (0 \u2264 \u03b3 \u2264 1)"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Goal"}),": Find optimal policy \u03c0* that maximizes expected cumulative reward:"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u03c0* = argmax E[\u03a3 \u03b3^t R_t]\n"})}),"\n",(0,s.jsx)(e.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Policy (\u03c0)"}),": Mapping from states to actions"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Deterministic"}),": a = \u03c0(s)"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Stochastic"}),": a ~ \u03c0(\xb7|s)"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Value Functions"}),":"]}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"State value"}),": V^\u03c0(s) = E[\u03a3 \u03b3^t R_t | s_0 = s]"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action value"}),": Q^\u03c0(s,a) = E[\u03a3 \u03b3^t R_t | s_0 = s, a_0 = a]"]}),"\n"]}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Bellman Equations"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"V(s) = max_a [R(s,a) + \u03b3 \u03a3 P(s'|s,a) V(s')]\nQ(s,a) = R(s,a) + \u03b3 \u03a3 P(s'|s,a) max_a' Q(s',a')\n"})}),"\n",(0,s.jsx)(e.h2,{id:"rl-algorithms-for-robotics",children:"RL Algorithms for Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"q-learning-off-policy",children:"Q-Learning (Off-Policy)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"Algorithm"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import numpy as np\n\nclass QLearning:\n    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n        self.Q = np.zeros((n_states, n_actions))\n        self.alpha = alpha  # Learning rate\n        self.gamma = gamma  # Discount factor\n        self.epsilon = epsilon  # Exploration rate\n\n    def select_action(self, state):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.Q.shape[1])  # Explore\n        return np.argmax(self.Q[state])  # Exploit\n\n    def update(self, state, action, reward, next_state):\n        td_target = reward + self.gamma * np.max(self.Q[next_state])\n        td_error = td_target - self.Q[state, action]\n        self.Q[state, action] += self.alpha * td_error\n\n# Training loop\nagent = QLearning(n_states=100, n_actions=4)\n\nfor episode in range(1000):\n    state = env.reset()\n    done = False\n\n    while not done:\n        action = agent.select_action(state)\n        next_state, reward, done = env.step(action)\n        agent.update(state, action, reward, next_state)\n        state = next_state\n"})}),"\n",(0,s.jsx)(e.h3,{id:"deep-q-network-dqn",children:"Deep Q-Network (DQN)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"For continuous state spaces"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\n\nclass DQN(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_network = DQN(state_dim, action_dim)\n        self.target_network = DQN(state_dim, action_dim)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-3)\n        self.memory = deque(maxlen=10000)\n        self.batch_size = 64\n        self.gamma = 0.99\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.01\n\n    def select_action(self, state):\n        if random.random() < self.epsilon:\n            return random.randint(0, self.q_network.network[-1].out_features - 1)\n\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            q_values = self.q_network(state_tensor)\n            return q_values.argmax().item()\n\n    def store_transition(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n\n    def train(self):\n        if len(self.memory) < self.batch_size:\n            return\n\n        batch = random.sample(self.memory, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(states)\n        actions = torch.LongTensor(actions)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(next_states)\n        dones = torch.FloatTensor(dones)\n\n        # Current Q values\n        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n\n        # Target Q values\n        with torch.no_grad():\n            max_next_q = self.target_network(next_states).max(1)[0]\n            target_q = rewards + (1 - dones) * self.gamma * max_next_q\n\n        # Loss and optimization\n        loss = nn.MSELoss()(current_q.squeeze(), target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # Decay epsilon\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n\n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n\n# Training\nagent = DQNAgent(state_dim=4, action_dim=2)\n\nfor episode in range(1000):\n    state = env.reset()\n    total_reward = 0\n\n    for step in range(200):\n        action = agent.select_action(state)\n        next_state, reward, done, _ = env.step(action)\n\n        agent.store_transition(state, action, reward, next_state, done)\n        agent.train()\n\n        state = next_state\n        total_reward += reward\n\n        if done:\n            break\n\n    if episode % 10 == 0:\n        agent.update_target_network()\n\n    print(f"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon:.3f}")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"State-of-the-art for robotics"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import torch\nimport torch.nn as nn\nfrom torch.distributions import Normal\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n\n        # Shared feature extractor\n        self.shared = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU()\n        )\n\n        # Actor (policy)\n        self.actor_mean = nn.Linear(256, action_dim)\n        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n\n        # Critic (value function)\n        self.critic = nn.Linear(256, 1)\n\n    def forward(self, state):\n        features = self.shared(state)\n\n        # Actor output\n        action_mean = self.actor_mean(features)\n        action_std = torch.exp(self.actor_log_std)\n\n        # Critic output\n        value = self.critic(features)\n\n        return action_mean, action_std, value\n\n    def get_action(self, state):\n        action_mean, action_std, value = self.forward(state)\n        dist = Normal(action_mean, action_std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n        return action, log_prob, value\n\nclass PPO:\n    def __init__(self, state_dim, action_dim):\n        self.policy = ActorCritic(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)\n\n        self.clip_epsilon = 0.2\n        self.gamma = 0.99\n        self.gae_lambda = 0.95\n        self.epochs = 10\n        self.batch_size = 64\n\n    def compute_gae(self, rewards, values, dones):\n        advantages = []\n        gae = 0\n\n        for t in reversed(range(len(rewards))):\n            if t == len(rewards) - 1:\n                next_value = 0\n            else:\n                next_value = values[t + 1]\n\n            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n            advantages.insert(0, gae)\n\n        return torch.FloatTensor(advantages)\n\n    def update(self, states, actions, old_log_probs, returns, advantages):\n        for _ in range(self.epochs):\n            # Get current policy outputs\n            action_mean, action_std, values = self.policy(states)\n            dist = Normal(action_mean, action_std)\n            log_probs = dist.log_prob(actions).sum(dim=-1)\n            entropy = dist.entropy().sum(dim=-1).mean()\n\n            # Compute ratios\n            ratios = torch.exp(log_probs - old_log_probs)\n\n            # Clipped surrogate objective\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n            actor_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            critic_loss = nn.MSELoss()(values.squeeze(), returns)\n\n            # Total loss\n            loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n\n            # Optimize\n            self.optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n            self.optimizer.step()\n\n# Training with PPO\nagent = PPO(state_dim=17, action_dim=6)  # For robot arm\n\nfor iteration in range(1000):\n    states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n\n    # Collect trajectories\n    state = env.reset()\n    for step in range(2048):\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action, log_prob, value = agent.policy.get_action(state_tensor)\n\n        next_state, reward, done, _ = env.step(action.detach().numpy()[0])\n\n        states.append(state)\n        actions.append(action.detach().numpy()[0])\n        rewards.append(reward)\n        dones.append(done)\n        log_probs.append(log_prob.item())\n        values.append(value.item())\n\n        state = next_state if not done else env.reset()\n\n    # Convert to tensors\n    states = torch.FloatTensor(states)\n    actions = torch.FloatTensor(actions)\n    old_log_probs = torch.FloatTensor(log_probs)\n\n    # Compute advantages and returns\n    advantages = agent.compute_gae(rewards, values, dones)\n    returns = advantages + torch.FloatTensor(values)\n\n    # Normalize advantages\n    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n    # Update policy\n    agent.update(states, actions, old_log_probs, returns, advantages)\n\n    print(f"Iteration {iteration}, Avg Reward: {sum(rewards) / len(rewards):.2f}")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"soft-actor-critic-sac",children:"Soft Actor-Critic (SAC)"}),"\n",(0,s.jsxs)(e.p,{children:[(0,s.jsx)(e.strong,{children:"For continuous control"}),":"]}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class SAC:\n    def __init__(self, state_dim, action_dim):\n        self.actor = Actor(state_dim, action_dim)\n        self.critic1 = Critic(state_dim, action_dim)\n        self.critic2 = Critic(state_dim, action_dim)\n\n        self.target_critic1 = Critic(state_dim, action_dim)\n        self.target_critic2 = Critic(state_dim, action_dim)\n\n        self.alpha = 0.2  # Temperature parameter\n        self.gamma = 0.99\n        self.tau = 0.005  # Soft update\n\n    def select_action(self, state, deterministic=False):\n        with torch.no_grad():\n            if deterministic:\n                return self.actor.get_mean_action(state)\n            return self.actor.sample_action(state)\n"})}),"\n",(0,s.jsx)(e.h2,{id:"reward-shaping-for-robotics",children:"Reward Shaping for Robotics"}),"\n",(0,s.jsx)(e.h3,{id:"manipulation-tasks",children:"Manipulation Tasks"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def compute_manipulation_reward(robot_state, object_state, target_state):\n    # Distance to object\n    dist_to_object = np.linalg.norm(robot_state['gripper_pos'] - object_state['pos'])\n    reach_reward = -dist_to_object\n\n    # Grasping reward\n    is_grasped = check_grasp(robot_state, object_state)\n    grasp_reward = 10.0 if is_grasped else 0.0\n\n    # Object to target distance\n    dist_to_target = np.linalg.norm(object_state['pos'] - target_state['pos'])\n    place_reward = -dist_to_target if is_grasped else 0.0\n\n    # Success bonus\n    success = dist_to_target < 0.05 and is_grasped\n    success_reward = 100.0 if success else 0.0\n\n    # Total reward\n    reward = reach_reward + grasp_reward + place_reward + success_reward\n\n    # Penalties\n    if robot_state['collision']:\n        reward -= 5.0\n\n    return reward\n"})}),"\n",(0,s.jsx)(e.h3,{id:"locomotion-tasks",children:"Locomotion Tasks"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"def compute_locomotion_reward(robot_state, target_velocity):\n    # Forward velocity reward\n    velocity_reward = robot_state['forward_velocity'] * 1.0\n\n    # Stability penalty (penalize falling)\n    height_penalty = -5.0 if robot_state['torso_height'] < 0.3 else 0.0\n\n    # Energy penalty\n    energy_penalty = -0.01 * np.sum(np.square(robot_state['joint_torques']))\n\n    # Orientation penalty (keep upright)\n    orientation_penalty = -0.5 * abs(robot_state['roll']) - 0.5 * abs(robot_state['pitch'])\n\n    # Deviation from target velocity\n    velocity_error = abs(robot_state['forward_velocity'] - target_velocity)\n    tracking_reward = -velocity_error\n\n    return velocity_reward + height_penalty + energy_penalty + orientation_penalty + tracking_reward\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class DomainRandomization:\n    def __init__(self):\n        self.params = {\n            'mass': (0.8, 1.2),\n            'friction': (0.5, 1.5),\n            'damping': (0.8, 1.2),\n            'motor_strength': (0.9, 1.1),\n            'latency': (0, 0.05)  # seconds\n        }\n\n    def randomize(self, env):\n        # Randomize physics\n        mass_scale = np.random.uniform(*self.params['mass'])\n        env.set_mass_scale(mass_scale)\n\n        friction = np.random.uniform(*self.params['friction'])\n        env.set_friction(friction)\n\n        # Randomize actuation\n        motor_strength = np.random.uniform(*self.params['motor_strength'])\n        env.set_motor_strength(motor_strength)\n\n        # Add latency\n        latency = np.random.uniform(*self.params['latency'])\n        env.set_action_latency(latency)\n\n        return env\n"})}),"\n",(0,s.jsx)(e.h3,{id:"residual-rl",children:"Residual RL"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"class ResidualPolicy:\n    def __init__(self, base_controller, rl_policy):\n        self.base = base_controller\n        self.rl = rl_policy\n\n    def get_action(self, state):\n        # Base controller (e.g., PD controller)\n        base_action = self.base.compute(state)\n\n        # RL residual\n        residual = self.rl.get_action(state)\n\n        # Combined action\n        return base_action + 0.1 * residual  # Scale residual\n"})}),"\n",(0,s.jsx)(e.h2,{id:"training-with-isaac-gym",children:"Training with Isaac Gym"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from isaacgymenvs.tasks.base.vec_task import VecTask\n\n# Train with rl_games\nfrom rl_games.torch_runner import Runner\n\nconfig = {\n    'params': {\n        'algo': {\n            'name': 'a2c_continuous'\n        },\n        'model': {\n            'name': 'continuous_a2c_logstd'\n        },\n        'network': {\n            'name': 'actor_critic',\n            'separate': False,\n            'space': {\n                'continuous': {\n                    'mu_activation': 'None',\n                    'sigma_activation': 'None',\n                    'mu_init': {\n                        'name': 'default'\n                    },\n                    'sigma_init': {\n                        'name': 'const_initializer',\n                        'val': 0\n                    },\n                    'fixed_sigma': True\n                }\n            },\n            'mlp': {\n                'units': [256, 128, 64],\n                'activation': 'elu',\n                'd2rl': False,\n                'initializer': {\n                    'name': 'default'\n                },\n                'regularizer': {\n                    'name': 'None'\n                }\n            }\n        },\n        'config': {\n            'name': 'FrankaCabinet',\n            'env_name': 'rlgpu',\n            'multi_gpu': False,\n            'ppo': True,\n            'mixed_precision': False,\n            'normalize_input': True,\n            'normalize_value': True,\n            'reward_shaper': {\n                'scale_value': 0.01\n            },\n            'num_actors': 2048,\n            'minibatch_size': 8192,\n            'mini_epochs': 8,\n            'horizon_length': 16,\n            'lr': 3e-4,\n            'max_epochs': 2000,\n            'save_best_after': 100,\n            'save_frequency': 100,\n            'gamma': 0.99,\n            'tau': 0.95\n        }\n    }\n}\n\nrunner = Runner()\nrunner.load(config)\nrunner.run({'train': True, 'play': False})\n"})}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"RL enables robots to learn complex behaviors through trial and error"}),"\n",(0,s.jsx)(e.li,{children:"PPO is state-of-the-art for continuous control robotics tasks"}),"\n",(0,s.jsx)(e.li,{children:"Reward shaping is critical for successful learning"}),"\n",(0,s.jsx)(e.li,{children:"Sim-to-real transfer requires domain randomization and physics variation"}),"\n",(0,s.jsx)(e.li,{children:"Isaac Gym enables massively parallel training (10-1000x speedup)"}),"\n",(0,s.jsx)(e.li,{children:"Stable-Baselines3 provides easy-to-use RL implementations"}),"\n",(0,s.jsx)(e.li,{children:"Curriculum learning helps tackle complex multi-stage tasks"}),"\n",(0,s.jsx)(e.li,{children:"Residual RL combines classical control with learned policies"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Stable-Baselines3"}),": ",(0,s.jsx)(e.a,{href:"https://stable-baselines3.readthedocs.io/",children:"https://stable-baselines3.readthedocs.io/"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Spinning Up in Deep RL"}),": ",(0,s.jsx)(e.a,{href:"https://spinningup.openai.com/",children:"https://spinningup.openai.com/"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Research Papers"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms"'}),"\n",(0,s.jsx)(e.li,{children:'Haarnoja, T., et al. (2018). "Soft Actor-Critic"'}),"\n",(0,s.jsx)(e.li,{children:'Makoviychuk, V., et al. (2021). "Isaac Gym: High Performance GPU-Based Physics Simulation"'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Books"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Sutton & Barto: "Reinforcement Learning: An Introduction"'}),"\n",(0,s.jsx)(e.li,{children:'Levine: "Reinforcement Learning and Control as Probabilistic Inference"'}),"\n"]}),"\n"]}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>i,x:()=>o});var a=t(6540);const s={},r=a.createContext(s);function i(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:i(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);