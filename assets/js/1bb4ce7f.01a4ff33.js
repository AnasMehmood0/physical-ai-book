"use strict";(globalThis.webpackChunkweb=globalThis.webpackChunkweb||[]).push([[1547],{1574:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"isaac/isaac-sdk","title":"01. Isaac SDK","description":"Platform for Robotics Simulation and Development","source":"@site/docs/isaac/isaac-sdk.md","sourceDirName":"isaac","slug":"/isaac/isaac-sdk","permalink":"/physical-ai-book/docs/isaac/isaac-sdk","draft":false,"unlisted":false,"editUrl":"https://github.com/AnasMehmood0/physical-ai-book/tree/main/web/docs/isaac/isaac-sdk.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"sidebar_label":"01. Isaac SDK"},"sidebar":"tutorialSidebar","previous":{"title":"02. Unity HRI","permalink":"/physical-ai-book/docs/simulation/unity-hri"},"next":{"title":"02. Reinforcement Learning","permalink":"/physical-ai-book/docs/isaac/reinforcement-learning"}}');var s=i(4848),r=i(8453);const o={sidebar_position:1,sidebar_label:"01. Isaac SDK"},t="01. Isaac SDK",l={},c=[{value:"Platform for Robotics Simulation and Development",id:"platform-for-robotics-simulation-and-development",level:2},{value:"Overview of NVIDIA Isaac Platform",id:"overview-of-nvidia-isaac-platform",level:2},{value:"Core Components",id:"core-components",level:3},{value:"Isaac Sim: High-Fidelity Simulation",id:"isaac-sim-high-fidelity-simulation",level:2},{value:"Why Isaac Sim?",id:"why-isaac-sim",level:3},{value:"Installation",id:"installation",level:3},{value:"Creating Your First Scene",id:"creating-your-first-scene",level:3},{value:"Loading and Controlling Robots",id:"loading-and-controlling-robots",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:3},{value:"Isaac Gym: Massively Parallel RL",id:"isaac-gym-massively-parallel-rl",level:2},{value:"Key Features",id:"key-features",level:3},{value:"Installation",id:"installation-1",level:3},{value:"Example: Training a Cartpole",id:"example-training-a-cartpole",level:3},{value:"Integration with Stable-Baselines3",id:"integration-with-stable-baselines3",level:3},{value:"Isaac ROS: GPU-Accelerated Perception",id:"isaac-ros-gpu-accelerated-perception",level:2},{value:"Key Packages",id:"key-packages",level:3},{value:"Installation",id:"installation-2",level:3},{value:"Example: Visual SLAM",id:"example-visual-slam",level:3},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Techniques",id:"techniques",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"GPU Utilization",id:"gpu-utilization",level:3},{value:"Best Practices",id:"best-practices",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"01-isaac-sdk",children:"01. Isaac SDK"})}),"\n",(0,s.jsx)(e.h2,{id:"platform-for-robotics-simulation-and-development",children:"Platform for Robotics Simulation and Development"}),"\n",(0,s.jsx)(e.p,{children:"This chapter introduces the NVIDIA Isaac SDK, a comprehensive platform designed to accelerate the development and deployment of AI-powered robots. It covers the core components of the SDK, including Isaac Sim (for realistic simulation), Isaac ROS (for integrating with ROS 2), and the various Isaac SDK GEMs (GPU-accelerated modules) for perception, navigation, and manipulation."}),"\n",(0,s.jsx)(e.h2,{id:"overview-of-nvidia-isaac-platform",children:"Overview of NVIDIA Isaac Platform"}),"\n",(0,s.jsx)(e.p,{children:"The Isaac platform is NVIDIA's end-to-end solution for robot development, combining simulation, AI training, and deployment tools optimized for NVIDIA GPUs."}),"\n",(0,s.jsx)(e.h3,{id:"core-components",children:"Core Components"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Isaac Sim:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Physically accurate simulation environment"}),"\n",(0,s.jsx)(e.li,{children:"Built on NVIDIA Omniverse platform"}),"\n",(0,s.jsx)(e.li,{children:"Photorealistic rendering with RTX ray tracing"}),"\n",(0,s.jsx)(e.li,{children:"High-fidelity physics simulation (PhysX 5)"}),"\n",(0,s.jsx)(e.li,{children:"Synthetic data generation for AI training"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Isaac Gym:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Massively parallel GPU-accelerated RL training"}),"\n",(0,s.jsx)(e.li,{children:"Thousands of simultaneous environment instances"}),"\n",(0,s.jsx)(e.li,{children:"Direct tensor-based physics API"}),"\n",(0,s.jsx)(e.li,{children:"10-1000x faster than traditional simulators"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Isaac ROS:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Hardware-accelerated ROS 2 packages"}),"\n",(0,s.jsx)(e.li,{children:"GPU-accelerated perception, localization, manipulation"}),"\n",(0,s.jsx)(e.li,{children:"Compatible with Jetson and desktop GPUs"}),"\n",(0,s.jsx)(e.li,{children:"Drop-in replacements for standard ROS packages"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Isaac SDK:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Modular robotics software framework"}),"\n",(0,s.jsx)(e.li,{children:"Navigation, manipulation, perception gems"}),"\n",(0,s.jsx)(e.li,{children:"Real-time performance on embedded systems"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"isaac-sim-high-fidelity-simulation",children:"Isaac Sim: High-Fidelity Simulation"}),"\n",(0,s.jsx)(e.h3,{id:"why-isaac-sim",children:"Why Isaac Sim?"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Advantages:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Photorealistic rendering"}),": RTX ray tracing for realistic lighting"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Accurate physics"}),": PhysX 5.1 for rigid bodies, soft bodies, fluids"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Synthetic data"}),": Labeled data for training vision models"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"USD integration"}),": Universal Scene Description for scene composition"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Multi-robot"}),": Simulate fleets of robots in shared environments"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ROS/ROS 2 native"}),": Built-in bridge for seamless integration"]}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Use Cases:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Training perception models with synthetic data"}),"\n",(0,s.jsx)(e.li,{children:"Testing navigation and manipulation algorithms"}),"\n",(0,s.jsx)(e.li,{children:"Multi-robot coordination scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Sim-to-real policy transfer"}),"\n",(0,s.jsx)(e.li,{children:"Digital twin development"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"System Requirements:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Ubuntu 20.04/22.04 or Windows 10/11"}),"\n",(0,s.jsx)(e.li,{children:"NVIDIA RTX GPU (2080 or better)"}),"\n",(0,s.jsx)(e.li,{children:"32GB RAM (64GB recommended)"}),"\n",(0,s.jsx)(e.li,{children:"50GB disk space"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Installation Steps:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Download from NVIDIA Omniverse Launcher\n# Install Omniverse Launcher first\nwget https://install.launcher.omniverse.nvidia.com/installers/omniverse-launcher-linux.AppImage\nchmod +x omniverse-launcher-linux.AppImage\n./omniverse-launcher-linux.AppImage\n\n# In Launcher, install Isaac Sim 2023.1.0+\n# Or via command line (after Launcher setup):\nomni_install isaac-sim\n"})}),"\n",(0,s.jsx)(e.h3,{id:"creating-your-first-scene",children:"Creating Your First Scene"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Python Script:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from isaacsim import SimulationApp\n\n# Launch Isaac Sim\nsimulation_app = SimulationApp({"headless": False})\n\nfrom omni.isaac.core import World\nfrom omni.isaac.core.objects import DynamicCuboid\nfrom omni.isaac.core.prims import RigidPrim\nimport numpy as np\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_default_ground_plane()\n\n# Add a cube\ncube = world.scene.add(\n    DynamicCuboid(\n        prim_path="/World/Cube",\n        name="cube",\n        position=np.array([0, 0, 1.0]),\n        scale=np.array([0.5, 0.5, 0.5]),\n        color=np.array([0.0, 0.0, 1.0])\n    )\n)\n\n# Reset world\nworld.reset()\n\n# Run simulation\nfor i in range(1000):\n    world.step(render=True)\n\n# Cleanup\nsimulation_app.close()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"loading-and-controlling-robots",children:"Loading and Controlling Robots"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Load Franka Robot:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from omni.isaac.franka import Franka\nfrom omni.isaac.core.utils.types import ArticulationAction\n\n# Create world\nworld = World(stage_units_in_meters=1.0)\nworld.scene.add_default_ground_plane()\n\n# Add Franka Panda robot\nfranka = world.scene.add(\n    Franka(prim_path="/World/Franka", name="franka")\n)\n\nworld.reset()\n\n# Control robot\nfor i in range(1000):\n    # Get current joint positions\n    joint_positions = franka.get_joint_positions()\n\n    # Command new positions (simple sine wave)\n    target_positions = joint_positions + 0.01 * np.sin(i * 0.01)\n\n    franka.apply_action(\n        ArticulationAction(joint_positions=target_positions)\n    )\n\n    world.step(render=True)\n\nsimulation_app.close()\n'})}),"\n",(0,s.jsx)(e.h3,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Generate Labeled Images:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from omni.isaac.core import World\nfrom omni.isaac.sensor import Camera\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\nimport omni.replicator.core as rep\n\n# Setup\nworld = World()\nworld.scene.add_default_ground_plane()\n\n# Add camera\ncamera = Camera(\n    prim_path="/World/Camera",\n    position=np.array([0, 0, 2.0]),\n    frequency=20,\n    resolution=(640, 480)\n)\n\n# Add objects to scene\nfor i in range(10):\n    cube = world.scene.add(\n        DynamicCuboid(\n            prim_path=f"/World/Cube_{i}",\n            position=np.random.uniform(-2, 2, 3),\n            scale=np.random.uniform(0.1, 0.5, 3)\n        )\n    )\n\nworld.reset()\n\n# Capture data\nwith rep.new_layer():\n    def capture():\n        with camera:\n            rgb = camera.get_rgba()\n            depth = camera.get_depth()\n            bbox = camera.get_bounding_box_2d()\n            segmentation = camera.get_semantic_segmentation()\n\n        return {\n            "rgb": rgb,\n            "depth": depth,\n            "bbox": bbox,\n            "segmentation": segmentation\n        }\n\n    for frame in range(100):\n        world.step(render=True)\n        data = capture()\n        # Save data\n        np.save(f"frame_{frame}_rgb.npy", data["rgb"])\n        np.save(f"frame_{frame}_depth.npy", data["depth"])\n\nsimulation_app.close()\n'})}),"\n",(0,s.jsx)(e.h2,{id:"isaac-gym-massively-parallel-rl",children:"Isaac Gym: Massively Parallel RL"}),"\n",(0,s.jsx)(e.p,{children:"Isaac Gym enables training RL policies 10-1000x faster than traditional simulators by running thousands of environments in parallel on GPU."}),"\n",(0,s.jsx)(e.h3,{id:"key-features",children:"Key Features"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"GPU-accelerated physics"}),": PhysX running entirely on GPU"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tensor API"}),": Direct PyTorch tensor access to simulation state"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Parallel environments"}),": 1,000-10,000+ simultaneous instances"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"No rendering overhead"}),": Headless training mode"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Integration"}),": Works with PyTorch, stable-baselines3, rl_games"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"installation-1",children:"Installation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Download from NVIDIA Isaac Gym Preview\nwget https://developer.nvidia.com/isaac-gym-preview-4\n\n# Extract and install\ntar -xf IsaacGym_Preview_4_Package.tar.gz\ncd isaacgym/python\npip install -e .\n\n# Test installation\npython examples/joint_monkey.py\n"})}),"\n",(0,s.jsx)(e.h3,{id:"example-training-a-cartpole",children:"Example: Training a Cartpole"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from isaacgym import gymapi\nfrom isaacgym import gymtorch\nimport torch\n\n# Create gym\ngym = gymapi.acquire_gym()\n\n# Simulation parameters\nsim_params = gymapi.SimParams()\nsim_params.dt = 1.0 / 60.0\nsim_params.substeps = 2\nsim_params.up_axis = gymapi.UP_AXIS_Z\nsim_params.gravity = gymapi.Vec3(0.0, 0.0, -9.81)\n\n# Physics engine\nsim_params.physx.solver_type = 1\nsim_params.physx.num_position_iterations = 4\nsim_params.physx.num_velocity_iterations = 1\nsim_params.physx.contact_offset = 0.01\nsim_params.physx.rest_offset = 0.0\n\n# Create sim\nsim = gym.create_sim(0, 0, gymapi.SIM_PHYSX, sim_params)\n\n# Create ground\nplane_params = gymapi.PlaneParams()\nplane_params.normal = gymapi.Vec3(0, 0, 1)\ngym.add_ground(sim, plane_params)\n\n# Load cartpole asset\nasset_root = "assets"\nasset_file = "cartpole.urdf"\nasset = gym.load_asset(sim, asset_root, asset_file)\n\n# Create environments\nnum_envs = 1024  # Parallel environments!\nenvs_per_row = 32\nenv_spacing = 2.0\n\nenvs = []\ncartpole_handles = []\n\nfor i in range(num_envs):\n    # Create env\n    env = gym.create_env(\n        sim,\n        gymapi.Vec3(-env_spacing, 0.0, -env_spacing),\n        gymapi.Vec3(env_spacing, env_spacing, env_spacing),\n        envs_per_row\n    )\n    envs.append(env)\n\n    # Add cartpole\n    pose = gymapi.Transform()\n    pose.p = gymapi.Vec3(0.0, 0.0, 2.0)\n\n    cartpole_handle = gym.create_actor(env, asset, pose, "cartpole", i, 1)\n    cartpole_handles.append(cartpole_handle)\n\n# Prepare for tensor API\ngym.prepare_sim(sim)\n\n# Get tensors\nroot_tensor = gymtorch.wrap_tensor(gym.acquire_actor_root_state_tensor(sim))\ndof_state_tensor = gymtorch.wrap_tensor(gym.acquire_dof_state_tensor(sim))\n\n# Training loop\nfor epoch in range(1000):\n    # Refresh tensors\n    gym.refresh_actor_root_state_tensor(sim)\n    gym.refresh_dof_state_tensor(sim)\n\n    # Observations (cart position, cart velocity, pole angle, pole velocity)\n    cart_pos = root_tensor[:, 0]\n    cart_vel = root_tensor[:, 7]\n    pole_angle = dof_state_tensor[:, 0]\n    pole_vel = dof_state_tensor[:, 1]\n\n    obs = torch.stack([cart_pos, cart_vel, pole_angle, pole_vel], dim=1)\n\n    # Policy (simple random for demonstration)\n    actions = torch.randn(num_envs, 1) * 10.0\n\n    # Apply actions\n    forces = torch.zeros(num_envs, 1, dtype=torch.float32)\n    forces[:, 0] = actions.squeeze()\n    gym.set_dof_actuation_force_tensor(sim, gymtorch.unwrap_tensor(forces))\n\n    # Step simulation\n    gym.simulate(sim)\n    gym.fetch_results(sim, True)\n\n    # Rewards\n    rewards = 1.0 - (pole_angle.abs() / 0.2)  # Keep pole upright\n\n    # Reset fallen poles\n    reset_mask = pole_angle.abs() > 0.5\n    if reset_mask.any():\n        # Reset logic here\n        pass\n\ngym.destroy_sim(sim)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"integration-with-stable-baselines3",children:"Integration with Stable-Baselines3"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'import gym\nfrom stable_baselines3 import PPO\nfrom isaacgymenvs.tasks import isaacgym_task_map\n\n# Create Isaac Gym environment\nenv_id = "Cartpole"\nnum_envs = 2048\n\nenv = isaacgym_task_map[env_id](\n    cfg={"env": {"numEnvs": num_envs}},\n    sim_device="cuda:0",\n    graphics_device_id=0,\n    headless=True\n)\n\n# Train with PPO\nmodel = PPO(\n    "MlpPolicy",\n    env,\n    verbose=1,\n    n_steps=16,\n    batch_size=2048,\n    learning_rate=3e-4,\n    device="cuda"\n)\n\nmodel.learn(total_timesteps=10_000_000)\nmodel.save("cartpole_ppo")\n'})}),"\n",(0,s.jsx)(e.h2,{id:"isaac-ros-gpu-accelerated-perception",children:"Isaac ROS: GPU-Accelerated Perception"}),"\n",(0,s.jsx)(e.p,{children:"Isaac ROS provides hardware-accelerated ROS 2 packages for perception, localization, and manipulation."}),"\n",(0,s.jsx)(e.h3,{id:"key-packages",children:"Key Packages"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"isaac_ros_visual_slam"}),": GPU-accelerated visual-inertial SLAM"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"isaac_ros_image_segmentation"}),": Real-time semantic segmentation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"isaac_ros_object_detection"}),": NVIDIA TAO-based object detection"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"isaac_ros_pose_estimation"}),": 6-DOF object pose estimation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"isaac_ros_stereo_depth"}),": Hardware-accelerated stereo depth"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"isaac_ros_apriltag"}),": Fast AprilTag detection"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"installation-2",children:"Installation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Prerequisites\nsudo apt-get install git-lfs\n\n# Clone Isaac ROS\nmkdir -p ~/workspaces/isaac_ros/src\ncd ~/workspaces/isaac_ros/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_common.git\n\n# Setup with Docker\ncd ~/workspaces/isaac_ros/src/isaac_ros_common\n./scripts/run_dev.sh\n\n# Inside container, clone desired packages\ncd /workspaces/isaac_ros/src\ngit clone https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam.git\n\n# Build\ncd /workspaces/isaac_ros\ncolcon build --symlink-install\nsource install/setup.bash\n"})}),"\n",(0,s.jsx)(e.h3,{id:"example-visual-slam",children:"Example: Visual SLAM"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Launch visual SLAM\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py\n\n# In another terminal, play a rosbag\nros2 bag play rosbag_data/\n\n# Visualize in RViz\nrviz2\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Python Node:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import Image\n\nclass SLAMProcessor(Node):\n    def __init__(self):\n        super().__init__('slam_processor')\n\n        # Subscribe to SLAM output\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            '/visual_slam/tracking/odometry',\n            self.odom_callback,\n            10\n        )\n\n        self.get_logger().info('SLAM processor started')\n\n    def odom_callback(self, msg):\n        pos = msg.pose.pose.position\n        self.get_logger().info(\n            f'Robot position: x={pos.x:.2f}, y={pos.y:.2f}, z={pos.z:.2f}'\n        )\n\ndef main():\n    rclpy.init()\n    node = SLAMProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,s.jsx)(e.p,{children:"Transferring policies from simulation to real robots is challenging. Isaac platform provides tools to minimize the reality gap."}),"\n",(0,s.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'from omni.isaac.core import World\nfrom omni.isaac.core.utils.prims import create_prim\nimport numpy as np\n\ndef randomize_lighting():\n    # Randomize light intensity\n    intensity = np.random.uniform(500, 2000)\n    # Set light parameters\n    pass\n\ndef randomize_textures():\n    # Randomize material properties\n    materials = ["wood", "metal", "plastic", "rubber"]\n    material = np.random.choice(materials)\n    # Apply material\n    pass\n\ndef randomize_physics():\n    # Randomize friction, mass, etc.\n    friction = np.random.uniform(0.3, 1.2)\n    mass_scale = np.random.uniform(0.8, 1.2)\n    # Apply physics parameters\n    pass\n\n# Training loop with domain randomization\nfor episode in range(num_episodes):\n    world.reset()\n\n    # Randomize at start of each episode\n    randomize_lighting()\n    randomize_textures()\n    randomize_physics()\n\n    # Run episode\n    for step in range(max_steps):\n        action = policy(observation)\n        observation, reward, done = world.step(action)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"techniques",children:"Techniques"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"1. Visual Diversity:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Randomize lighting (intensity, color, position)"}),"\n",(0,s.jsx)(e.li,{children:"Randomize textures and materials"}),"\n",(0,s.jsx)(e.li,{children:"Randomize camera parameters (FOV, exposure, noise)"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"2. Physics Randomization:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Mass and inertia variation (\xb120%)"}),"\n",(0,s.jsx)(e.li,{children:"Friction coefficients (0.5x to 1.5x)"}),"\n",(0,s.jsx)(e.li,{children:"Joint damping and stiffness"}),"\n",(0,s.jsx)(e.li,{children:"Action delays and latency"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"3. Sensor Noise:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Camera noise (Gaussian, salt-and-pepper)"}),"\n",(0,s.jsx)(e.li,{children:"Depth sensor noise and dropouts"}),"\n",(0,s.jsx)(e.li,{children:"IMU bias and drift"}),"\n",(0,s.jsx)(e.li,{children:"GPS multipath errors"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"4. Curriculum Learning:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Start with easy, predictable scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Gradually increase randomization"}),"\n",(0,s.jsx)(e.li,{children:"Increase task complexity over time"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(e.h3,{id:"gpu-utilization",children:"GPU Utilization"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# Monitor GPU usage\nimport pynvml\n\npynvml.nvmlInit()\nhandle = pynvml.nvmlDeviceGetHandleByIndex(0)\n\ndef print_gpu_stats():\n    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n\n    print(f"GPU Memory: {info.used / 1024**3:.2f} GB / {info.total / 1024**3:.2f} GB")\n    print(f"GPU Utilization: {utilization.gpu}%")\n'})}),"\n",(0,s.jsx)(e.h3,{id:"best-practices",children:"Best Practices"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"1. Batch Operations:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# Bad: Process images one at a time\nfor img in images:\n    result = model(img)\n\n# Good: Batch processing\nresults = model(torch.stack(images))\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"2. Async Data Loading:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from torch.utils.data import DataLoader\n\n# Use multiple workers for data loading\ndataloader = DataLoader(\n    dataset,\n    batch_size=256,\n    num_workers=4,\n    pin_memory=True,  # Faster GPU transfer\n    prefetch_factor=2\n)\n"})}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"3. Mixed Precision:"})}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor epoch in range(num_epochs):\n    with autocast():\n        output = model(input)\n        loss = criterion(output, target)\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n"})}),"\n",(0,s.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Isaac Sim provides photorealistic simulation with RTX ray tracing and PhysX physics"}),"\n",(0,s.jsx)(e.li,{children:"Isaac Gym enables massively parallel RL training (1000+ environments on single GPU)"}),"\n",(0,s.jsx)(e.li,{children:"Isaac ROS offers hardware-accelerated perception packages for real-time performance"}),"\n",(0,s.jsx)(e.li,{children:"Synthetic data generation reduces need for real-world labeled data"}),"\n",(0,s.jsx)(e.li,{children:"Domain randomization and curriculum learning enable effective sim-to-real transfer"}),"\n",(0,s.jsx)(e.li,{children:"GPU optimization is critical for real-time performance on embedded systems (Jetson)"}),"\n",(0,s.jsx)(e.li,{children:"USD format enables scene composition and collaboration"}),"\n",(0,s.jsx)(e.li,{children:"Isaac platform integrates with ROS 2, PyTorch, and standard ML frameworks"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Isaac Sim Documentation"}),": ",(0,s.jsx)(e.a,{href:"https://docs.omniverse.nvidia.com/isaacsim/",children:"https://docs.omniverse.nvidia.com/isaacsim/"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Isaac Gym"}),": ",(0,s.jsx)(e.a,{href:"https://developer.nvidia.com/isaac-gym",children:"https://developer.nvidia.com/isaac-gym"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Isaac ROS"}),": ",(0,s.jsx)(e.a,{href:"https://nvidia-isaac-ros.github.io/",children:"https://nvidia-isaac-ros.github.io/"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Research Papers"}),":","\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:'Makoviychuk, V., et al. (2021). "Isaac Gym: High Performance GPU-Based Physics Simulation"'}),"\n",(0,s.jsx)(e.li,{children:'Tobin, J., et al. (2017). "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"'}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"NVIDIA Developer Blog"}),": ",(0,s.jsx)(e.a,{href:"https://developer.nvidia.com/blog/robotics/",children:"https://developer.nvidia.com/blog/robotics/"})]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Tutorials"}),": ",(0,s.jsx)(e.a,{href:"https://github.com/NVIDIA-Omniverse/IsaacSim-tutorial",children:"https://github.com/NVIDIA-Omniverse/IsaacSim-tutorial"})]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var a=i(6540);const s={},r=a.createContext(s);function o(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);